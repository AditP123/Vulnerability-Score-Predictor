#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, ElasticNet, Lars, Lasso, OrthogonalMatchingPursuit, ARDRegression, MultiTaskElasticNet
from sklearn.kernel_ridge import KernelRidge
from sklearn import svm
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, median_absolute_error


# In[2]:


df = pd.read_csv("../../Datasets/Supervised Learning Inputs/ATP_TI_MCAS_AADP.csv")


# In[3]:


df = df.dropna()
df.info()


# In[4]:


# X = df.iloc[:, 0:6] # not taking threat count
X = df[['UPN Count']] #  taking upn and threat count
Y = df[['AvgProbability']]
scaler = StandardScaler()


# In[5]:


X.info()
Y.info()


# In[6]:


# Y.head()


# In[7]:


x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)


# ### Defining Models

# In[20]:


linear_regression = LinearRegression() # Linear Regression Model
ridge_regression = Ridge(alpha=0.5)
polynomial_regression = LinearRegression()
svr = svm.SVR(C=1.0, epsilon=0.2)
decision_tree = DecisionTreeRegressor(max_depth=50)
random_forest = RandomForestRegressor(n_estimators=10, max_features=1, max_leaf_nodes=20,random_state=42)
sgd_regressor = SGDRegressor(max_iter=1000, tol=1e-3)
elastic_net = ElasticNet(random_state=0)
lars = Lars(n_nonzero_coefs=1)
lasso = Lasso(alpha=0.1)
omp_regression = OrthogonalMatchingPursuit()
ard_regression = ARDRegression()
# multitask_elastic_net = MultiTaskElasticNet(alpha=0.1)
krr = krr = KernelRidge(alpha=1.0)





poly_features = PolynomialFeatures(degree=2)


# In[21]:


predictions_df = pd.DataFrame()
metrics_df = pd.DataFrame(columns=["Model", "MAE", "MSE", "R-squared", "RMSE", "Median AE"])
# metrics_df.set_index('Model', inplace=True)
# predictions_df["UPN"] = x_test['UPN']
predictions_df["Real Predictions"] = y_test

model_map = {
    linear_regression: "Linear Regression",
    ridge_regression: "Ridge Regression",
    polynomial_regression: "Polynomial Regression",
    svr: "Support Vector Regressor",
    decision_tree: "Decision Tree Regressor",
    random_forest: "Random Forest Regressor",
    sgd_regressor: "Stochastic Gradient Descent Regressor",
    elastic_net: "Elastic Net Regressor",
    lars: "Lars",
    lasso: "Lasso",
    omp_regression: "Orthogonal Matching Pursuit Regressor",
    ard_regression: "Bayesian ARD Regression",
    # multitask_elastic_net: "Multitask Elastic Net Regressor",
    krr: "Kernel Ridge Regressor"
}


# In[22]:


def metrics(df, model, y_test, predictions):
    metric_list = [model_map[model], mean_absolute_error(y_test, predictions), mean_squared_error(y_test, predictions), r2_score(y_test, predictions),  np.sqrt(mean_squared_error(y_test, predictions)), median_absolute_error(y_test, predictions) ]
    print(metric_list)
    df.loc[len(df)] = metric_list
    


# In[23]:


models = [linear_regression, ridge_regression, polynomial_regression, svr, decision_tree, random_forest, sgd_regressor, lars, lasso, omp_regression, ard_regression,  krr] #multitask_elastic_net,



def model_test_eval(model,x_train,x_test,y_train,y_test):
    if model == polynomial_regression:
        model.fit(poly_features.fit_transform(x_train), y_train)
        predictions = model.predict(poly_features.fit_transform(x_test))
        predictions_df[f"{model_map[model]}  Predictions"] = predictions
        metrics(metrics_df, model, y_test, predictions)

    elif model == linear_regression:
        model.fit(np.array(x_train), y_train)
        predictions = model.predict(np.array(x_test))
        predictions_df[f"{model_map[model]} Predictions"] = predictions
        metrics(metrics_df, model, y_test, predictions)

    else:

        model.fit(scaler.fit_transform(x_train), np.array(y_train).ravel())
        predictions = model.predict(scaler.fit_transform(x_test))
        predictions_df[f"{model_map[model]} Predictions"] = predictions
        metrics(metrics_df, model, np.array(y_test).ravel(), predictions)


# ### Training Models

# In[24]:


for model in models:
    model_test_eval(model,x_train,x_test,y_train,y_test)


# In[25]:


predictions_df.head()


# In[26]:


predictions_df.describe()


# In[27]:


pd.options.display.float_format = '{:.05f}'.format
metrics_df.head(15)


# ##### SGD Regressors seems to be the most suitable fit to the given dataset with low MAE, MSE, RMSE values and with a positive R-squared value

# In[28]:


# import pickle

# # save
# for model in models:
#     with open(f'../../../Backend/model_api/models/Supervised/{model_map[model]}.pkl','wb') as f:
#         pickle.dump(model,f)


# ### Visualising data points

# In[29]:


get_ipython().run_line_magic('matplotlib', 'inline')
plt.xlabel('Threat Count')
plt.ylabel('AvgProbability Score')
plt.scatter(x_test.iloc[:,-1:], y_test, color='red', label="Actual")
plt.scatter(x_test.iloc[:,-1:], predictions_df['Stochastic Gradient Descent Regressor Predictions'],  color='blue', label="Predicted")
plt.title("SGD Regressor Plot")
# plt.plot(predictions_df['Stochastic Gradient Descent Regressor Predictions'])
plt.legend()


# In[ ]:


df.info()


# In[ ]:


corr = pd.DataFrame(scaler.fit_transform(df.iloc[:,2:])).corr()


# In[ ]:


corr


# In[ ]:




