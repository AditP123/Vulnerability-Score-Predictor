#!/usr/bin/env python
# coding: utf-8

# In[187]:


import os 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer


# In[188]:


DATASET_1_PATH = "Datasets"
filename = "MLInput_MCAS_AADP.csv"
filepath = os.path.join(DATASET_1_PATH, filename)


# In[189]:


df = pd.read_csv("../Datasets/MLInput_MCAS_AADP.csv", encoding = "cp1252")


# In[190]:


df.head()


# In[191]:


df.describe()


# In[192]:


df.info()


# In[193]:


df.drop(df.tail(1).index,inplace = True)


# In[194]:


df.tail()


# In[195]:


df.info()


# In[196]:


missingrows = df.isna().any(axis = 1)


# In[197]:


missingrows


# In[198]:


df.loc[missingrows].index


# In[199]:


df.loc[464]


# In[200]:


df.loc[463]


# In[201]:


df.loc[464, "Description New"] = "Suspicious email" 


# In[202]:


df.info()


# In[203]:


df.loc[464]


# In[204]:


desc_count  = df["Description New"].value_counts()


# In[205]:


desc_count


# In[206]:


desc_count = desc_count.to_dict()


# In[207]:


desc_count


# In[208]:


plt.subplots(2,1,figsize = (10,10))
plt.subplot(2,1,1)
plt.pie(list(desc_count.values()), labels = list(desc_count.keys()))
plt.legend()

plt.subplot(2,1,2)
plt.bar(list(desc_count.keys()), list(desc_count.values()))
plt.show()


# In[209]:


location_count = df["Location"].value_counts()
location_count = location_count.to_dict()

plt.figure(figsize = (15,15))
plt.subplot(2,1,1)
plt.pie(list(location_count.values()), labels = None)
plt.legend(list(location_count.keys()))

plt.subplot(2,1,2)
plt.bar(list(location_count.keys()), list(location_count.values()))
plt.show()


# In[210]:


ip_count = df["IP"].value_counts()
ip_count = ip_count.to_dict()

plt.figure(figsize = (15,15))
plt.pie(list(ip_count.values()), labels = None)
plt.legend(list(ip_count.keys()))

plt.show()


# In[211]:


#Counter function to count the number of instances
def counter(df,feature):
    df[f'{feature} Count'] = df[feature].groupby(df[feature]).transform('size')
    return df


# In[217]:


clean_df = counter(df,'UPN')
clean_df = counter(df,'IP')
clean_df.info()


# In[218]:


# Convert Date column to datetime format
clean_df['Date'] = pd.to_datetime(clean_df['Date'], errors='coerce')

# Extract time-based features
clean_df['Hour'] = clean_df['Date'].dt.hour
clean_df['DayOfWeek'] = clean_df['Date'].dt.dayofweek
clean_df['Month'] = clean_df['Date'].dt.month


# In[219]:


# One-hot encoding for Severity
# clean_df = pd.get_dummies(clean_df, columns=['Severity'])

# TF-IDF vectorization for Description
vectorizer = TfidfVectorizer(max_features=100)  # Limiting to top 100 features for simplicity
tfidf_matrix = vectorizer.fit_transform(clean_df['Description New'])

# Convert the TF-IDF matrix to a DataFrame and concatenate with the original data
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())
clean_df = pd.concat([clean_df, tfidf_df], axis=1)


# In[220]:


clean_df.head()


# In[221]:


clean_df.to_csv('../Datasets/MLInput_MCAS_AADP_cleaned_unsupervised.csv')


# ### Cleaning ATP TI Dataset for Supervised Model

# In[238]:


df = pd.read_csv("../Datasets/MLInput_ATP_TI.csv", encoding = "cp1252")
op_df = pd.read_csv("../Datasets/ATP_TI_output.csv", encoding = "cp1252")
op_df


# In[239]:


new_df = df.copy()
new_df = new_df.assign(UPN=new_df['UPN'].str.split(';')).explode('UPN')


# In[240]:


# Group by 'UPN' and count the occurrences of each 'Description' for each user
description_count_per_user = new_df.groupby('UPN')['Description New'].value_counts().unstack(fill_value=0)

# Reset the index to make 'UPN' a column again
description_count_per_user.reset_index(inplace=True)

# Display the result
description_count_per_user

merged_data = pd.merge(new_df, description_count_per_user, on='UPN', how='left')


# In[245]:


merged_data = counter(merged_data,'UPN')
merged_data['UPN'].value_counts()


# In[246]:


merged_data['Name'] = merged_data['UPN'].str.split('@').str[0]


# In[247]:


expanded_data = op_df.assign(UPN=op_df['UPN'].str.split(';')).explode('UPN')
expanded_data = expanded_data.groupby('UPN')['AvgProbability'].mean().reset_index()
expanded_data['Name'] = expanded_data['UPN'].str.split('@').str[0]


# In[260]:


expanded_data.to_csv("../Datasets/ATP_TI_output_cleaned.csv")


# In[249]:


final_data = pd.merge(merged_data, expanded_data[['Name','AvgProbability']], on = "Name", how = "left")


# In[250]:


final_data


# In[251]:


final_data = final_data.round({"AvgProbability": 4})


# In[252]:


final_data.to_csv("../Datasets/MLInput_ATP_TI_cleaned_supervised_reference_notToBeTested.csv")


# In[253]:


new_df


# In[254]:


final_data


# In[255]:


final_df = final_data[['UPN','Brand impersonation','Malicious URL reputation', 'Malware', 'Threats','UPN Count','AvgProbability']]


# In[256]:


final_df


# In[257]:


final_df = final_df.groupby('UPN').agg({
    'AvgProbability': 'mean',
    'UPN Count': 'mean',
    'Brand impersonation': 'mean',
    'Malicious URL reputation': 'mean',
    'Malware': 'mean',
    'Threats': 'mean'
}).reset_index()


# In[258]:


final_df = final_df.round({"AvgProbability": 4})


# In[259]:


final_df.to_csv("../Datasets/MLInput_ATP_TI_cleaned_supervised.csv")

