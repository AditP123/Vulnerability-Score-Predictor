#!/usr/bin/env python
# coding: utf-8

# In[5]:


import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.cluster import HDBSCAN
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np


# In[3]:


df_mcas = pd.read_csv("../Datasets/MLInput_MCAS_AADP_cleaned.csv", encoding = "cp1252")
df_atp = pd.read_csv("../Datasets/MLInput_ATP_TI_cleaned.csv", encoding = "cp1252")


# In[4]:


df_mcas['UPN Count NS'] = df_mcas['UPN Count']
df_atp['UPN Count NS'] = df_atp['UPN Count']


# ### Using MCAS Dataset

# In[6]:


scaler = StandardScaler()
features = ['IP Count','UPN Count','delete','deleted','email','failed','folder','from','items','log','messages','on','suspicious']
for feature in features:
    df_mcas[feature] = scaler.fit_transform(df_mcas[[feature]])


# In[8]:


X_mcas = df_mcas[['IP Count','UPN Count','delete','deleted','email','failed','folder','from','items','log','messages','on','suspicious']]


# In[24]:


hdb_mcas = HDBSCAN(min_cluster_size=3).fit(X_mcas)
mcas_cluster_labels = hdb_mcas.labels_
df_mcas['cluster'] = hdb_mcas.labels_
X_mcas['cluster_labels'] = hdb_mcas.labels_


# In[27]:


def score(new_df):
    # Calculate the mean UPN Count for each cluster
    group_means = new_df.groupby('cluster')['UPN Count'].mean().reset_index()

    # Sort clusters by mean UPN Count in ascending order
    sorted_groups = group_means.sort_values(by='UPN Count', ascending=True).reset_index(drop=True)

    # Create a mapping of cluster to vulnerability score
    vulnerability_score_mapping = {cluster: score + 1 for score, cluster in enumerate(sorted_groups['cluster'])}

    # Add a new column 'vulnerability_score' to the original DataFrame
    new_df['vulnerability_score'] = new_df['cluster'].map(vulnerability_score_mapping)
    
    # Normalize the vulnerability scores (excluding outliers)
    max_score = new_df['vulnerability_score'][new_df['vulnerability_score'] > 0].max()
    new_df['vulnerability_score'] = new_df['vulnerability_score'] / max_score
    
    return new_df

def metrics(X, cluster_labels):
    filtered_X = X[cluster_labels != -1]
    filtered_labels = cluster_labels[cluster_labels != -1]
    # print(filtered_labels)

    if len(set(filtered_labels)) > 1:  # Need at least 2 clusters for these metrics
        silhouette_avg = silhouette_score(filtered_X, filtered_labels)
        davies_bouldin = davies_bouldin_score(filtered_X, filtered_labels)
        calinski_harabasz = calinski_harabasz_score(filtered_X, filtered_labels)

        print(f'Silhouette Score: {silhouette_avg}')
        print(f'Davies-Bouldin Index: {davies_bouldin}')
        print(f'Calinski-Harabasz Index: {calinski_harabasz}')
    else:
        print('Not enough clusters for metric calculations.')


# In[38]:


score(df_mcas)
print(df_mcas[['UPN', 'UPN Count NS', 'vulnerability_score']])
metrics(X_mcas,mcas_cluster_labels)


# ### Using ATP TI Dataset

# In[40]:


features = ['IP Count','UPN Count',"Description New Count"]
for feature in features:
    df_atp[feature] = scaler.fit_transform(df_atp[[feature]])


# In[41]:


X_atp = df_atp[["IP Count", "UPN Count", "Description New Count"]]


# In[43]:


hdb_atp = HDBSCAN(min_cluster_size=4).fit(X_atp)
atp_cluster_labels = hdb_atp.labels_
df_atp['cluster'] = hdb_atp.labels_
X_atp['cluster_labels'] = hdb_atp.labels_


# In[44]:


score(df_atp)
print(df_atp[['UPN', 'UPN Count NS', 'vulnerability_score']])
metrics(X_atp,atp_cluster_labels)


# In[ ]:




