#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np
import ipaddress


# In[2]:


df = pd.read_csv("../Datasets/MLInput_MCAS_AADP_cleaned.csv", encoding = "cp1252")


# In[3]:


df.head()


# In[5]:


df.info()


# In[11]:


def ip_to_int(ip):
    try:
        return int(ipaddress.ip_address(ip))
    except ValueError:
        return np.nan


# In[13]:


df['IP_num'] = df['IP'].apply(ip_to_int)


# In[44]:


df['IP_num']


# In[229]:


pt= RobustScaler()
df['IP_scaled'] = pt.fit_transform(df[['IP Count']])
df['UPN_scaled'] = pt.fit_transform(df[['UPN Count']])


# In[231]:


df['IP_scaled'].min()


# In[155]:


df['Location']


# In[169]:


lb = LabelEncoder()
df['Location_encoded'] = lb.fit_transform(df['Location'])


# In[170]:


df['Location_encoded']


# In[239]:


X = df[['IP_scaled','UPN_scaled','Location_encoded','delete','deleted','email','failed','folder','from','items','log','messages','on','suspicious']]
# 'UPN Count','Location_encoded'


# In[240]:


clustering = DBSCAN(eps=0.5, min_samples=2).fit(X)


# In[241]:


clustering.labels_


# In[242]:


df['cluster'] = clustering.labels_


# In[243]:


# Analyzing clusters and assign vulnerability scores based on cluster size, marking outliers with -1
cluster_sizes = df['cluster'].value_counts().to_dict()
df['vulnerability_score'] = df['cluster'].map(lambda x: cluster_sizes[x] if x != -1 else 0)


# In[244]:


# Normalize the vulnerability scores (excluding outliers)
max_score = df['vulnerability_score'][df['vulnerability_score'] > 0].max()
df['vulnerability_score'] = df['vulnerability_score'] / max_score


# In[245]:


# Display the dataframe with assigned vulnerability scores
print(df[['UPN', 'UPN Count','vulnerability_score']])


# In[246]:


# Calculate clustering metrics
# Filter out noise points for metric calculations
filtered_X = X[cluster_labels != -1]
filtered_labels = cluster_labels[cluster_labels != -1]
print(filtered_labels)

if len(set(filtered_labels)) > 1:  # Need at least 2 clusters for these metrics
    silhouette_avg = silhouette_score(filtered_X, filtered_labels)
    davies_bouldin = davies_bouldin_score(filtered_X, filtered_labels)
    calinski_harabasz = calinski_harabasz_score(filtered_X, filtered_labels)

    print(f'Silhouette Score: {silhouette_avg}')
    print(f'Davies-Bouldin Index: {davies_bouldin}')
    print(f'Calinski-Harabasz Index: {calinski_harabasz}')
else:
    print('Not enough clusters for metric calculations.')


# In[247]:


# Visualization with PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(10, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.6)
plt.title('DBSCAN Clusters (PCA-reduced)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(label='Cluster')
plt.show()


# In[248]:


# Visualization with t-SNE
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

plt.figure(figsize=(10, 6))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels, cmap='viridis', s=50, alpha=0.6)
plt.title('DBSCAN Clusters (t-SNE)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.colorbar(label='Cluster')
plt.show()


# ### Scaling all Features

# In[4]:


new_df = df.iloc[:,:].copy()
scaler = StandardScaler()
features = ['IP Count','UPN Count','delete','deleted','email','failed','folder','from','items','log','messages','on','suspicious']
for feature in features:
    new_df[feature] = scaler.fit_transform(df[[feature]])


# In[5]:


new_df.head()


# In[6]:


X_new = new_df[['IP Count','UPN Count','delete','deleted','email','failed','folder','from','items','log','messages','on','suspicious']]


# In[26]:


clustering_2 = DBSCAN(eps=0.5, min_samples=2).fit(X_new)
cluster_labels_2 = clustering_2.labels_
new_df['cluster'] = clustering_2.labels_

# Analyzing clusters and assign vulnerability scores based on cluster size, marking outliers with -1
cluster_sizes = new_df['cluster'].value_counts().to_dict()
new_df['vulnerability_score'] = new_df['cluster'].map(lambda x: cluster_sizes[x] if x != -1 else 0)

# Normalize the vulnerability scores (excluding outliers)
max_score = new_df['vulnerability_score'][new_df['vulnerability_score'] > 0].max()
new_df['vulnerability_score'] = new_df['vulnerability_score'] / max_score


# In[20]:


new_df['cluster'].min()


# In[22]:


X_new['cluster_labels'] = new_df['cluster']


# In[8]:


# Display the dataframe with assigned vulnerability scores
print(new_df[['UPN','vulnerability_score']])


# In[9]:


# Calculate the mean UPN Count for each cluster
group_means = new_df.groupby('cluster')['UPN Count'].mean().reset_index()

# Sort clusters by mean UPN Count in ascending order
sorted_groups = group_means.sort_values(by='UPN Count', ascending=True).reset_index(drop=True)

# Create a mapping of cluster to vulnerability score
vulnerability_score_mapping = {cluster: score + 1 for score, cluster in enumerate(sorted_groups['cluster'])}

# Add a new column 'vulnerability_score' to the original DataFrame
new_df['vulnerability_score'] = new_df['cluster'].map(vulnerability_score_mapping)


# In[10]:


new_df['UPN Count'] = df['UPN Count']


# In[11]:


print(new_df[['UPN', 'UPN Count','vulnerability_score']])


# In[94]:


new_df.to_csv("../Datasets/MLInput_MCAS_AADP_DBSCAN.csv")


# In[29]:


# Calculate clustering metrics
# Filter out noise points for metric calculations
filtered_X_new = X_new[cluster_labels_2 != -1]
filtered_labels = cluster_labels_2[cluster_labels_2 != -1]
# print(filtered_labels)

if len(set(filtered_labels)) > 1:  # Need at least 2 clusters for these metrics
    silhouette_avg = silhouette_score(filtered_X_new, filtered_labels)
    davies_bouldin = davies_bouldin_score(filtered_X_new, filtered_labels)
    calinski_harabasz = calinski_harabasz_score(filtered_X_new, filtered_labels)

    print(f'Silhouette Score: {silhouette_avg}')
    print(f'Davies-Bouldin Index: {davies_bouldin}')
    print(f'Calinski-Harabasz Index: {calinski_harabasz}')
else:
    print('Not enough clusters for metric calculations.')


# In[30]:


# Visualization with PCA
pca = PCA(n_components=2)
X_new_pca = pca.fit_transform(X_new)

plt.figure(figsize=(10, 6))
plt.scatter(X_new_pca[:, 0], X_new_pca[:, 1], c=cluster_labels_2, cmap='viridis', s=50, alpha=0.6)
plt.title('DBSCAN Clusters (PCA-reduced)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(label='Cluster')
plt.show()


# In[31]:


# Visualization with t-SNE
tsne = TSNE(n_components=2, random_state=42)
X_new_tsne = tsne.fit_transform(X_new)

plt.figure(figsize=(10, 6))
plt.scatter(X_new_tsne[:, 0], X_new_tsne[:, 1], c=cluster_labels_2, cmap='viridis', s=50, alpha=0.6)
plt.title('DBSCAN Clusters (t-SNE)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.colorbar(label='Cluster')
plt.show()


# In[34]:


import pickle

# save
with open('../Models/Unsupervised/DBSCAN_MCAS_AADP.pkl','wb') as f:
    pickle.dump(clustering_2,f)


# ## Using ATP TI Dataset

# In[88]:


df_2 = pd.read_csv("../Datasets/MLInput_ATP_TI_cleaned.csv", encoding = "cp1252")


# In[89]:


df_2.info()
df_2['UPN Count NS'] = df_2['UPN Count']


# In[90]:


# scaler = StandardScaler()
# features = ['IP Count''UPN Count',"Description New Count"]
# for feature in features:
#     df_2[feature] = scaler.fit_transform(df_2[[feature]])


# In[91]:


# X_2 = df_2[["IP Count", "UPN Count", "Hour", "DayOfWeek", "Month", "Description New_Brand impersonation",
#             "Description New_Malicious URL reputation","Description New_Malware", "Description New_Threats"]]
X_2 = df_2[["IP Count", "UPN Count", "Description New Count"]]

clustering_ATP = DBSCAN(eps=0.5, min_samples=2).fit(X_2)
cluster_labels_ATP = clustering_ATP.labels_
df_2['cluster'] = clustering_ATP.labels_

# Analyzing clusters and assign vulnerability scores based on cluster size, marking outliers with -1
cluster_sizes = df_2['cluster'].value_counts().to_dict()
df_2['vulnerability_score'] = df_2['cluster'].map(lambda x: cluster_sizes[x] if x != -1 else 0)

# Normalize the vulnerability scores (excluding outliers)
max_score = df_2['vulnerability_score'][df_2['vulnerability_score'] > 0].max()
df_2['vulnerability_score'] = df_2['vulnerability_score'] / max_score


# In[92]:


df_2.head()


# In[93]:


# Display the dataframe with assigned vulnerability scores
df_2[['UPN','UPN Count NS','vulnerability_score']]


# In[95]:


# Calculate clustering metrics
# Filter out noise points for metric calculations
filtered_X_2 = X_2[cluster_labels_ATP != -1]
filtered_labels = cluster_labels_ATP[cluster_labels_ATP != -1]
# print(filtered_labels)

if len(set(filtered_labels)) > 1:  # Need at least 2 clusters for these metrics
    silhouette_avg = silhouette_score(filtered_X_2, filtered_labels)
    davies_bouldin = davies_bouldin_score(filtered_X_2, filtered_labels)
    calinski_harabasz = calinski_harabasz_score(filtered_X_2, filtered_labels)

    print(f'Silhouette Score: {silhouette_avg}')
    print(f'Davies-Bouldin Index: {davies_bouldin}')
    print(f'Calinski-Harabasz Index: {calinski_harabasz}')
else:
    print('Not enough clusters for metric calculations.')


# In[97]:


# Visualization with PCA
pca = PCA(n_components=2)
X_2_pca = pca.fit_transform(X_2)

plt.figure(figsize=(10, 6))
plt.scatter(X_2_pca[:, 0], X_2_pca[:, 1], c=cluster_labels_ATP, cmap='viridis', s=50, alpha=0.6)
plt.title('DBSCAN Clusters (PCA-reduced)')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(label='Cluster')
plt.show()


# In[99]:


# Visualization with t-SNE
tsne = TSNE(n_components=2, random_state=42)
X_2_tsne = tsne.fit_transform(X_2)

plt.figure(figsize=(10, 6))
plt.scatter(X_2_tsne[:, 0], X_2_tsne[:, 1], c=cluster_labels_ATP, cmap='viridis', s=50, alpha=0.6)
plt.title('DBSCAN Clusters (t-SNE)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.colorbar(label='Cluster')
plt.show()


# In[ ]:




