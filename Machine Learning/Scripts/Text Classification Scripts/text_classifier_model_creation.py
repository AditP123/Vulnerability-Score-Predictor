
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)
print()

# Load the tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
# Load the model
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)

df = pd.read_csv('/kaggle/input/qsc-threat-classification/Text_Classification_Input_Cleaned.csv')
df.head()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

df['Description Encoded'] = le.fit_transform(df['Description Consolidated'])

# @title Description Consolidated

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('Description Consolidated').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

X_train, x_test, y_train, y_test = train_test_split(df['Description'], df['Description Encoded'], test_size=0.2, random_state=42)
x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)

for value in [x_train, x_val, x_test, y_train, y_val, y_test]:
    print(len(value))
    
train_df = Dataset.from_pandas(pd.DataFrame({'text': x_train, 'label': y_train}))
val_df = Dataset.from_pandas(pd.DataFrame({'text': x_val, 'label': y_val}))
test_df = Dataset.from_pandas(pd.DataFrame({'text': x_test, 'label': y_test}))

# Tokenize the data
def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

train_tokenized_df = train_df.map(tokenize_function, batched=True)
val_tokenized_df = val_df.map(tokenize_function, batched=True)
test_tokenized_df = test_df.map(tokenize_function, batched=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    report_to = "none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized_df,
    eval_dataset=val_tokenized_df,
)

# Train the model
trainer.train()

y_pred = trainer.predict(test_tokenized_df)

type(y_pred)

predicted_labels = np.argmax(y_pred.predictions, axis=1)

from sklearn.metrics import accuracy_score
import numpy as np
accuracy_score(np.array(y_test), predicted_labels)

trainer.save_model(output_dir="./models")




