#!/usr/bin/env python
# coding: utf-8

# In[117]:


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, Lars, Lasso, OrthogonalMatchingPursuit, ARDRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor 
from sklearn.ensemble import RandomForestRegressor

# Step 1: Load the dataset
file_path = '../../Datasets/Supervised Learning Inputs/MLInput_ATP_TI_cleaned_supervised.csv'
df = pd.read_csv(file_path, encoding = "cp1252")
test_df = df.copy()
test_df = test_df.dropna()


# In[106]:


X = test_df.drop(['Unnamed: 0','UPN','AvgProbability'], axis = 1)
y = test_df['AvgProbability']


# In[107]:


### Linear Regression
linear_regression = LinearRegression()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
linear_regression.fit(X_train,y_train)
y_pred = linear_regression.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
test_mse = mean_squared_error(y_test, y_pred)
test_r2 = r2_score(y_test, y_pred)
print(test_mae)
print(test_mse)
print(test_r2)


# In[108]:


### Ridge Regression
ridge_regression = Ridge(alpha=0.5)
ridge_regression.fit(X_train,y_train)
y_pred = ridge_regression.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
test_mse = mean_squared_error(y_test, y_pred)
test_r2 = r2_score(y_test, y_pred)
print(test_mae)
print(test_mse)
print(test_r2)


# In[109]:


### Polynomial Regression
poly = PolynomialFeatures(degree = 2)
X_poly = poly.fit_transform(X)
X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly, y, test_size=0.2, random_state=42)
polynomial_regression = LinearRegression()
polynomial_regression.fit(X_train_poly,y_train_poly)
y_pred = polynomial_regression.predict(X_test_poly)
test_mae = mean_absolute_error(y_test_poly, y_pred)
test_mse = mean_squared_error(y_test_poly, y_pred)
test_r2 = r2_score(y_test_poly, y_pred)
print(test_mae)
print(test_mse)
print(test_r2)


# In[110]:


### Support Vector Regression
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()

support_vector_regression = SVR(kernel = 'poly') # You can use different kernels like 'linear', 'poly', 'rbf'
X_train_svr, X_test_svr, y_train_svr, y_test_svr = train_test_split(X_scaled, y_scaled, test_size = 0.2, random_state = 42)
support_vector_regression.fit(X_train_svr, y_train_svr)
y_pred = support_vector_regression.predict(X_test_svr)
test_mae = mean_absolute_error(y_test_svr, y_pred)
test_mse = mean_squared_error(y_test_svr, y_pred)
test_r2 = r2_score(y_test_svr, y_pred)
print(test_mae)
print(test_mse)
print(test_r2)


# In[111]:


### Decision Tree Regression
decision_tree_regressor = DecisionTreeRegressor(random_state=42)
decision_tree_regressor.fit(X_train, y_train)
y_pred = decision_tree_regressor.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
test_mse = mean_squared_error(y_test, y_pred)
test_r2 = r2_score(y_test, y_pred)
print(test_mae)
print(test_mse)
print(test_r2)


# In[112]:


### Random Forest Regression

random_forest_regressor = RandomForestRegressor(random_state=42)
random_forest_regressor.fit(X_train, y_train)
y_pred = decision_tree_regressor.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
test_mse = mean_squared_error(y_test, y_pred)
test_r2 = r2_score(y_test, y_pred)
print(test_mae)
print(test_mse)
print(test_r2)


# In[113]:


### Stochastic Gradient Descent Regression

scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

sgd_regressor = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)
sgd_regressor.fit(X_train, y_train)

y_pred = sgd_regressor.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
test_mse = mean_squared_error(y_test, y_pred)
test_r2 = r2_score(y_test, y_pred)
print(test_mae)
print(test_mse)
print(test_r2)


# In[114]:


### LARS

scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

lars_regressor = Lars()
lars_regressor.fit(X_train, y_train)
y_pred = lars_regressor.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
test_mse = mean_squared_error(y_test, y_pred)
test_r2 = r2_score(y_test, y_pred)
print(test_mae)
print(test_mse)
print(test_r2)


# In[115]:


### Lasso

scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

# Normalize the target variable
scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

# Train the Lasso regressor
lasso_regressor = Lasso(alpha=1.0, random_state=42)  # You can adjust the alpha parameter as needed
lasso_regressor.fit(X_train, y_train)

# Evaluate the model
y_pred= lasso_regressor.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
test_mse = mean_squared_error(y_test, y_pred)
test_r2 = r2_score(y_test, y_pred)
print(test_mae)
print(test_mse)
print(test_r2)


# In[116]:


### Orthogonal Matching Pursuit Regressor
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

# Normalize the target variable
scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

# Train the OMP regressor
omp_regressor = OrthogonalMatchingPursuit()
omp_regressor.fit(X_train, y_train)

# Evaluate the model
y_pred= omp_regressor.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
test_mse = mean_squared_error(y_test, y_pred)
test_r2 = r2_score(y_test, y_pred)
print(test_mae)
print(test_mse)
print(test_r2)


# In[118]:


### Bayesian ARD Regression
scaler_X = StandardScaler()
X_scaled = scaler_X.fit_transform(X)

# Normalize the target variable
scaler_y = StandardScaler()
y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)

# Train the Bayesian ARD regressor
ard_regressor = ARDRegression()
ard_regressor.fit(X_train, y_train)

# Evaluate the model
y_pred = ard_regressor.predict(X_test)
test_mae = mean_absolute_error(y_test, y_pred)
test_mse = mean_squared_error(y_test, y_pred)
test_r2 = r2_score(y_test, y_pred)
print(test_mae)
print(test_mse)
print(test_r2)

